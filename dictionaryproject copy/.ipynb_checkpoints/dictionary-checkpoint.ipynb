{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cleaning the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"673.txt\", 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for cleaning the definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes examples from definitions\n",
    "def remove_eg(definition):\n",
    "    if(definition.find('<as>') != -1):\n",
    "        definition = definition[:definition.find('<as>')] + definition[definition.find('</as>')+5:]\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the definition of any markers such as <i>, </i> or similar. All markers are recognized by the brackets.\n",
    "def clean_def(definition):\n",
    "    inBrackets = False\n",
    "    newDefinition = ''\n",
    "    for letter in definition:\n",
    "        if(letter == '<'):\n",
    "            inBrackets = True\n",
    "        if(not inBrackets):\n",
    "            newDefinition += letter\n",
    "        if(letter == '>'):\n",
    "            inBrackets = False\n",
    "    return newDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns a definition into the set of words in that definition which are also in the dictionary\n",
    "def set_of_words(sentence, dictionary):\n",
    "    words = sentence.split()\n",
    "    finalSet = set()\n",
    "    for word in words:\n",
    "        # for words which end in a single punctuation.\n",
    "        if not word.isalpha(): \n",
    "            word = word[:-1]  \n",
    "        # We only include words that are in our dictionary. Words \n",
    "        # that are undefined are assumed to be unimportant.\n",
    "        if word.lower() in dictionary: \n",
    "            finalSet.add(word.lower())\n",
    "    return finalSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling a python dictionary with words from Webster's. Each word maps to a set of words that are in its definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_dictionary = {} \n",
    "\n",
    "i = 0 #begin reading from 0 position in file\n",
    "\n",
    "while(text.find('<h1>',i) != -1):  # first pass through for words\n",
    "    a = text.find('<h1>', i)       # We recognize words by the <h1> marker\n",
    "    b = text.find('</h1>', i)\n",
    "    word = text[a+4:b]\n",
    "    word = word.lower()\n",
    "    i = b+5\n",
    "    if word.isalpha():             # we only include normal words in our dictionary\n",
    "        the_dictionary[word] = set()\n",
    "\n",
    "i = 0\n",
    "\n",
    "while(text.find('<h1>',i) != -1):  # Second pass through for definitions\n",
    "    a = text.find('<h1>', i)\n",
    "    b = text.find('</h1>', i)\n",
    "    word = text[a+4:b]\n",
    "    word = word.lower()\n",
    "    i = b+5\n",
    "    if word.isalpha():             # we only include normal words in our dictionary\n",
    "        while(text.find('<def>', i) < text.find('<h1>',i)):\n",
    "            c = text.find('<def>', i)\n",
    "            d = text.find('</def>', i)\n",
    "            if text.find(' See <', c) < d and text.find(' See <', c) != 0:\n",
    "                d = text.find(' See <')\n",
    "            i = text.find('</def>', i)+4\n",
    "            definition_str = text[c+5:d]\n",
    "            definition_str = remove_eg(definition_str) # Remove example so fewer words reference themselves\n",
    "            definition_str = clean_def(definition_str) # Get rid of other tags\n",
    "            definition_set = set_of_words(definition_str, the_dictionary) # set of words in the remaining definition\n",
    "            the_dictionary[word].update(definition_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have effectively a directed graph of all words in the dictionary, where each edge points from a word to a word that is in that word's definition.\n",
    "Plan for finding a (not necessarily the smallest) generative vocabulary: find all cycles. Remove words until no cycles remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a cycle starting from word 'start', using the dictionary as a graph and ignoring 'invalid' words\n",
    "# which are words we already assume we know. If a cycle is found, it returns a word in that cycle. If no \n",
    "# cycle is found, then we can define every word in 'visited', so we return the set visited.\n",
    "def find_cycle(start, dictionary, invalid):\n",
    "    visited = set()\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        curr = stack.pop()\n",
    "        if curr not in invalid:\n",
    "            if curr in visited:\n",
    "                return curr\n",
    "            visited.add(curr)\n",
    "            for word in dictionary[curr]:\n",
    "                stack.append(word)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as 'find_cycle'. Method #2 to try to get a smaller generative set.\n",
    "def find_cycle2(start, dictionary, invalid):\n",
    "    visited = set()\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        curr = stack.pop()\n",
    "        if curr not in invalid:\n",
    "            if curr in visited:\n",
    "                return curr\n",
    "            if curr in the_dictionary[curr]:\n",
    "                return curr\n",
    "            else:\n",
    "                visited.add(curr)\n",
    "                for word in dictionary[curr]:\n",
    "                    stack.append(word)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as 'find_cycle'. Returns the word in the cycle.\n",
    "import queue\n",
    "\n",
    "def find_cycle3(start, dictionary, invalid):\n",
    "    visited = set()\n",
    "    q = queue.SimpleQueue()\n",
    "    q.put(start)\n",
    "    while not q.empty():\n",
    "        curr = q.get(False)\n",
    "        if curr not in invalid:\n",
    "            if curr in visited:\n",
    "                return curr\n",
    "            else:\n",
    "                visited.add(curr)\n",
    "                for word in dictionary[curr]:\n",
    "                    q.put(word)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as 'find_cycle'. Returns the word with the longest definition\n",
    "def find_cycle4(start, dictionary, invalid):\n",
    "    visited = []\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        curr = stack.pop()\n",
    "        if curr not in invalid:\n",
    "            if curr in visited:\n",
    "                cycle = visited[visited.index(curr):]\n",
    "                return max(cycle, key=lambda x: len(dictionary.get(x)))\n",
    "            visited.append(curr)\n",
    "            for word in dictionary[curr]:\n",
    "                stack.append(word)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFS that checks each word's own definition so that any words that contain themselves\n",
    "# Size: 10,852\n",
    "def find_cycle5(start, dictionary, invalid):\n",
    "    visited = set()\n",
    "    q = queue.SimpleQueue()\n",
    "    q.put(start)\n",
    "    while not q.empty():\n",
    "        curr = q.get(False)\n",
    "        if curr not in invalid:\n",
    "            if curr in visited:\n",
    "                return curr\n",
    "            if curr in dictionary[curr]:\n",
    "                return curr\n",
    "            else:\n",
    "                visited.add(curr)\n",
    "                for word in dictionary[curr]:\n",
    "                    q.put(word)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10852\n"
     ]
    }
   ],
   "source": [
    "gen_vocab = set()\n",
    "done = set()\n",
    "\n",
    "for word in the_dictionary:\n",
    "    while word not in done:\n",
    "        x = find_cycle5(word, the_dictionary, done)\n",
    "        if type(x) == str:\n",
    "            gen_vocab.add(x)\n",
    "            done.add(x)\n",
    "        else:\n",
    "            done.update(x)\n",
    "print(len(gen_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best strategy I could find: \n",
    "1. Check for 1-cycles and add all members to the gen_vocab\n",
    "2. Check for 2-cycles and add most-used words to the gen_vocab\n",
    "3. Use a BFS to find other cycles and add words to the gen_vocab until all words can be defined\n",
    "## Main Barriers to improvement\n",
    "It seems like the \"best\" strategy to find the smallest generative vocabulary would be to find cycles of increasing size (continuing my own strategy with 3-cycles, 4-cycles...). But this would involve finding all cycles, and the best known algorithms for finding all cycles in a directed graph are O((e+v)(c+1)). Since c is likely exponential in v, this algorithm becomes unrealistic for a graph with 85,000 vertices and 966845 edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10750\n"
     ]
    }
   ],
   "source": [
    "gen_vocab = set()\n",
    "\n",
    "# 1. Find all 1-cycles\n",
    "oneCycles = {word for word in the_dictionary if word in the_dictionary[word]}\n",
    "gen_vocab.update(oneCycles)\n",
    "\n",
    "# 2. Find a word in each 2-cycle\n",
    "twoCycles = set()\n",
    "times = defaultdict(int)        # Map from all words in 2-cycles to the number of cycles they're in\n",
    "for word in the_dictionary:\n",
    "    if word not in oneCycles:\n",
    "        for word2 in the_dictionary[word]:\n",
    "            if word2 not in oneCycles:\n",
    "                if word in the_dictionary[word2]:\n",
    "                    times[word] += 1\n",
    "\n",
    "while(True):\n",
    "    newWord = max(times,key=times.get)    # Get's the most-used word\n",
    "    if times[newWord] == 0:\n",
    "        break\n",
    "    twoCycles.add(newWord)\n",
    "    for word in the_dictionary[newWord]:\n",
    "        if newWord in the_dictionary[word]:\n",
    "            times[word] -= 1\n",
    "            times[newWord] -= 1\n",
    "\n",
    "gen_vocab.update(twoCycles)\n",
    "done = gen_vocab.copy()\n",
    "\n",
    "for word in the_dictionary:\n",
    "    while word not in done:\n",
    "        x = find_cycle3(word, the_dictionary, done)\n",
    "        if type(x) == str:\n",
    "            gen_vocab.add(x)\n",
    "            done.add(x)\n",
    "        else:\n",
    "            done.update(x)\n",
    "print(len(gen_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966845\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "for word in the_dictionary:\n",
    "    e += len(the_dictionary[word])\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
